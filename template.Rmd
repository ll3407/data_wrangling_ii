---
title: "Data Manipulation with `dplyr`"
output: github_document
---

```{r, echo=FALSE}
library(tidyverse)

knitr::opts_chunk$set(
  collapse = TRUE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
```
options(tibble.print_min = 3)

```{r}
litters_data =
  read_csv("./data/FAS_litters.csv")
litters_data = 
  janitor::clean_names(litters_data)

pups_data = read_csv("./data/FAS_pups.csv")
pups_data = janitor::clean_names(pups_data)
```

### `select`

For a given analysis, you may only need a subset of the columns in a data table; extracting only what you need can helpfully de-clutter, especially when you have large datasets. Select columns using `select`.

You can specify the columns you want to keep by naming all of them:

```{r}
select(litters_data, group, litter_number, gd0_weight, pups_born_alive)
```

You can specify the specify a range of columns to keep:

```{r}
select(litters_data, group:gd_of_birth)
```

You can also specify columns you'd like to remove:
```{r}
select(litters_data, -pups_survive)
```

You can rename variables as part of this process:

```{r}
select(litters_data, GROUP = group, LiTtEr_NuMbEr = litter_number)
```

If all you want to do is rename something, you can use `rename` instead of `select`. This will rename the variables you care about, and keep everything else:

```{r}
rename(litters_data, GROUP = group, LiTtEr_NuMbEr = litter_number)
```

There are some handy helper functions for `select`; read about all of them using `?select_helpers`. I use `starts_with()`, `ends_with()`, and `contains()` often, especially when there variables are named with suffixes or other standard patterns:

```{r}
select(litters_data,starts_with("gd"))
select(litters_data, ends_with("weight"))
```
I also frequently use is `everything()`, which is handy for reorganizing columns without discarding anything:

```{r}
select(litters_data, litter_number, pups_survive, everything())
```

Lastly, like other functions in `dplyr`, `select` will export a dataframe even if you only select one column. Mostly this is fine, but sometimes you want the vector stored in the column. To pull a single variable, use `pull`.


### `filter`

Some data tables will include rows you don't need for your current analysis. Although you could remove specific row numbers using base R, you shouldn't -- this might break if the raw data are updated, and the thought process isn't transparent. Instead, you should filter rows based on logical expressions using the `filter` function. Like `select`, the first argument to `filter` is the dataframe you're filtering; all subsequent arguments are logical expressions.

You will often filter using comparison operators (`>`, `>=`, `<`, `<=`, `==`, and `!=`). You may also use `%in%` to detect if values appear in a set, and `is.na()` to find missing values. The results of comparisons are logical -- the statement is `TRUE` or `FALSE` depending on the values you compare -- and can be combined with other comparisons using the logical operators `&` and `|`, or negated using `!`. 

Some ways you might filter the litters data are:

* `gd_of_birth == 20`
* `pups_born_alive >= 2`
* `pups_survive != 4`
* `!(pups_survive == 4)` _is the same with the previous one, but is better in the following situation_ 
`!((pups_survive == 4) & (gd_of_birth == 20))`

* `group %in% c("Con7", "Con8")`
* `group == "Con7" & gd_of_birth == 20`

Let's try one or two... 
```{r}
filter(litters_data, gd_of_birth == 20)
```
```{r}
filter(litters_data, group == "Con7" & gd_of_birth == 20)
```
A very common filtering step requires you to omit missing observations. You *can* do this with `filter`, but I recommend using `drop_na` from the `tidyr` package:

* `drop_na(litters_data)` will remove any row with a missing value
* `drop_na(litters_data, wt_increase)` will remove rows for which `wt_increase` is missing. 

Filtering can be helpful for limiting a dataset to only those observations needed for an analysis. However, I recommend against the creation of many data subsets (e.g. one for each group). This can clutter up your workspace, and we'll see good tools for the analysis of subsets before long.

### `mutate`

Sometimes you need to select columns; sometimes you need to change them or create new ones. You can do this using `mutate`. 

The example below creates a new variable measuring the difference between `gd18_weight` and `gd0_weight` and modifies the existing `group` variable.

```{r}
#litter_data2 =
mutate(litters_data,
  wt_gain = gd18_weight - gd0_weight,
  group = str_to_lower(group),
 # wt_gain_kg = wt_gain * 2.2
)
```
A few things in this example are worth noting:

* Your new variables can be functions of old variables
* New variables appear at the end of the dataset in the order that they are created
* You can overwrite old variables
* You can create a new variable and immediately refer to (or change) it

Creating a new variable that does exactly what you need can be a challenge; the more functions you know about, the easier this gets. 

### `arrange` _is like sort in SAS_

In comparison to the preceding, arranging is pretty straightforward. You can arrange the rows in your data according to the values in one or more columns:

```{r}
head(arrange(litters_data, group, pups_born_alive), 10)
```

You can also sort in descending order if you'd like.
```{r}
head(arrange(litters_data, desc(group), pups_born_alive), 10)
```
### `%>%`

We've seen several commands you will use regularly for data manipulation and cleaning. You will rarely use them in isolation. For example, suppose you want to load the data, clean the column names, remove `pups_survive`, and create `wt_gain`. There are a couple of options for this kind of multi-step data manipulation:

* define intermediate datasets (or overwrite data at each stage)
* nest function calls

The following is an example of the first option:

```{r}
litters_data_raw = read_csv("./data/FAS_litters.csv",
  col_types = "ccddiiii")
litters_data_clean_names = janitor::clean_names(litters_data_raw)
litters_data_selected_cols = select(litters_data_clean_names, -pups_survive)
litters_data_with_vars = 
  mutate(
    litters_data_selected_cols, 
    wt_gain = gd18_weight - gd0_weight,
    group = str_to_lower(group))
litters_data_with_vars_without_missing = 
  drop_na(litters_data_with_vars, wt_gain)
litters_data_with_vars_without_missing
```

Below, we try the second option:

```{r}
litters_data_clean = 
  drop_na(
    mutate(
      select(
        janitor::clean_names(
          read_csv("./data/FAS_litters.csv", col_types = "ccddiiii")
          ), 
      -pups_survive
      ),
    wt_gain = gd18_weight - gd0_weight,
    group = str_to_lower(group)
    ),
  wt_gain
  )
litters_data_clean
```

These are both confusing and bad: the first gets confusing and clutters our workspace, and the second has to be read inside out.

Piping solves this problem. It allows you to turn the nested approach into a sequential chain by passing the result of one function call as an argument to the next function call:  _use short cut ctrl+shift+M_

```{r}
litters_data = 
  read_csv("./data/FAS_litters.csv", col_types = "ccddiiii") %>%
  janitor::clean_names() %>%
  select(-pups_survive) %>% 
  mutate(
    wt_gain = gd18_weight - gd0_weight,
    group = str_to_lower(group)) %>% 
  drop_na(wt_gain)
litters_data
```

show with the non tidyverse code
```{r}
litters_data %>% 
  lm(wt_gain ~ pups_born_alive, data = .) %>% 
  broom::tidy()

```

